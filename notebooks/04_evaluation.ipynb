{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Comprehensive Model Evaluation\n\n**Models:** Base, SFT Trial 1 & 2, DPO Trial 1 & 2\n\n**Output:** `results/evaluation_results.json` with all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers peft trl bitsandbytes accelerate sacrebleu nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from sacrebleu.metrics import BLEU\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.makedirs('results', exist_ok=True)\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Evaluation Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('evaluation/eval_prompts.json', 'r') as f:\n",
    "    eval_prompts = json.load(f)\n",
    "print(f\"Loaded {len(eval_prompts)} evaluation prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Model paths\n",
    "MODEL_PATHS = {\n",
    "    \"base\": None,\n",
    "    \"sft_trial1\": \"./outputs/sft_trial1/final\",\n",
    "    \"sft_trial2\": \"./outputs/sft_trial2/final\",\n",
    "    \"dpo_trial1\": \"./outputs/dpo_trial1/final\",\n",
    "    \"dpo_trial2\": \"./outputs/dpo_trial2/final\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(adapter_path=None):\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=bnb_config, device_map=\"auto\")\n",
    "    if adapter_path:\n",
    "        model = PeftModel.from_pretrained(model, adapter_path)\n",
    "    return model\n",
    "\n",
    "def generate_response(model, prompt, max_tokens=256):\n",
    "    sys_tok = \"<\" + \"|system|\" + \">\"\n",
    "    usr_tok = \"<\" + \"|user|\" + \">\"\n",
    "    ast_tok = \"<\" + \"|assistant|\" + \">\"\n",
    "    eos = \"<\" + \"/s\" + \">\"\n",
    "    \n",
    "    formatted = f\"{sys_tok}\\nYou are a helpful assistant.{eos}\\n{usr_tok}\\n{prompt}{eos}\\n{ast_tok}\\n\"\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_tokens, temperature=0.7, top_p=0.9, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    if ast_tok in response:\n",
    "        response = response.split(ast_tok)[-1].replace(eos, \"\").strip()\n",
    "    return response\n",
    "\n",
    "bleu = BLEU(effective_order=True)\n",
    "def calc_bleu(hyp, ref):\n",
    "    return bleu.sentence_score(hyp, [ref]).score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "\n",
    "for model_name, model_path in MODEL_PATHS.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Evaluating: {model_name}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    try:\n",
    "        model = load_model(model_path)\n",
    "        results = []\n",
    "        \n",
    "        for p in eval_prompts:\n",
    "            response = generate_response(model, p['prompt'])\n",
    "            bleu_score = calc_bleu(response, p['target_response'])\n",
    "            \n",
    "            results.append({\n",
    "                'prompt_id': p['id'],\n",
    "                'category': p['category'],\n",
    "                'prompt': p['prompt'],\n",
    "                'target_response': p['target_response'],\n",
    "                'model_response': response,\n",
    "                'bleu_score': bleu_score\n",
    "            })\n",
    "            print(f\"  Prompt {p['id']}: BLEU={bleu_score:.2f}\")\n",
    "        \n",
    "        all_results[model_name] = {\n",
    "            'results': results,\n",
    "            'avg_bleu': sum(r['bleu_score'] for r in results) / len(results)\n",
    "        }\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        all_results[model_name] = {'error': str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Summary JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results JSON\n",
    "evaluation_output = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"num_prompts\": len(eval_prompts),\n",
    "    \"models_evaluated\": list(all_results.keys()),\n",
    "    \"summary\": {},\n",
    "    \"detailed_results\": {}\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BLEU SCORE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<20} {'Avg BLEU':<15}\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "for model_name, data in all_results.items():\n",
    "    if 'error' not in data:\n",
    "        avg = data['avg_bleu']\n",
    "        evaluation_output[\"summary\"][model_name] = {\n",
    "            \"avg_bleu\": avg,\n",
    "            \"per_prompt_scores\": {r['prompt_id']: r['bleu_score'] for r in data['results']}\n",
    "        }\n",
    "        evaluation_output[\"detailed_results\"][model_name] = data['results']\n",
    "        print(f\"{model_name:<20} {avg:<15.2f}\")\n",
    "\n",
    "# Save comprehensive JSON\n",
    "with open('results/evaluation_results.json', 'w') as f:\n",
    "    json.dump(evaluation_output, f, indent=2)\n",
    "print(f\"\\nResults saved to results/evaluation_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comparison dataframe\n",
    "rows = []\n",
    "for model_name, data in all_results.items():\n",
    "    if 'results' in data:\n",
    "        for r in data['results']:\n",
    "            rows.append({\n",
    "                'Model': model_name,\n",
    "                'Prompt ID': r['prompt_id'],\n",
    "                'Category': r['category'],\n",
    "                'BLEU': r['bleu_score']\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"\\nBLEU Scores by Prompt:\")\n",
    "pivot = df.pivot(index='Prompt ID', columns='Model', values='BLEU')\n",
    "print(pivot.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sample Response Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE RESPONSES (Prompt 1)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model_name, data in all_results.items():\n",
    "    if 'results' in data:\n",
    "        print(f\"\\n### {model_name} (BLEU: {data['results'][0]['bleu_score']:.2f}) ###\")\n",
    "        print(data['results'][0]['model_response'][:400])\n",
    "        print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Manual Evaluation Template (for DPO models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate manual evaluation JSON template\n",
    "manual_eval_template = {\n",
    "    \"evaluator\": \"YOUR_NAME\",\n",
    "    \"date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    \"evaluation_criteria\": {\n",
    "        \"helpfulness\": \"How helpful is the response? (1-5)\",\n",
    "        \"harmlessness\": \"Is the response safe and appropriate? (1-5)\",\n",
    "        \"relevance\": \"How well does it address the prompt? (1-5)\"\n",
    "    },\n",
    "    \"evaluations\": []\n",
    "}\n",
    "\n",
    "for p in eval_prompts:\n",
    "    for model in [\"dpo_trial1\", \"dpo_trial2\"]:\n",
    "        if model in all_results and 'results' in all_results[model]:\n",
    "            response = [r for r in all_results[model]['results'] if r['prompt_id'] == p['id']][0]\n",
    "            manual_eval_template[\"evaluations\"].append({\n",
    "                \"prompt_id\": p['id'],\n",
    "                \"model\": model,\n",
    "                \"response_preview\": response['model_response'][:200],\n",
    "                \"helpfulness\": None,\n",
    "                \"harmlessness\": None,\n",
    "                \"relevance\": None,\n",
    "                \"notes\": \"\"\n",
    "            })\n",
    "\n",
    "with open('results/manual_evaluation_template.json', 'w') as f:\n",
    "    json.dump(manual_eval_template, f, indent=2)\n",
    "print(\"Manual evaluation template saved to results/manual_evaluation_template.json\")\n",
    "print(\"Fill in the scores (1-5) for each response!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  - results/evaluation_results.json (all BLEU scores & responses)\")\n",
    "print(\"  - results/manual_evaluation_template.json (for DPO manual eval)\")\n",
    "print(\"\\nAll training results:\")\n",
    "print(\"  - results/sft_trial1_results.json\")\n",
    "print(\"  - results/sft_trial2_results.json\")\n",
    "print(\"  - results/dpo_trial1_results.json\")\n",
    "print(\"  - results/dpo_trial2_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}